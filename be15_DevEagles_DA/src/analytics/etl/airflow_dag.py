"""Airflow DAG for customer analytics ETL pipeline."""

from datetime import datetime, timedelta
from typing import Dict, Any
import os
import requests

try:
    from airflow import DAG
    from airflow.operators.python import PythonOperator
    from airflow.operators.bash import BashOperator
    from airflow.sensors.filesystem import FileSensor
    from airflow.utils.dates import days_ago
    from airflow.utils.task_group import TaskGroup
    AIRFLOW_AVAILABLE = True
except ImportError:
    AIRFLOW_AVAILABLE = False

from analytics.core.logging import get_logger
from analytics.core.config import settings
from .config import DEFAULT_AIRFLOW_CONFIG, DEFAULT_ETL_CONFIG
from .pipeline import create_pipeline


logger = get_logger("AirflowDAG")


def run_etl_pipeline(**context) -> Dict[str, Any]:
    """ETL íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ íƒœìŠ¤í¬."""
    import asyncio
    
    # ì‹¤í–‰ ë‚ ì§œ ì¶”ì¶œ
    execution_date = context['execution_date']
    incremental = context.get('dag_run').conf.get('incremental', True)
    use_spark = context.get('dag_run').conf.get('use_spark', False)
    
    logger.info(f"Starting ETL pipeline - execution_date: {execution_date}, incremental: {incremental}")
    
    # íŒŒì´í”„ë¼ì¸ ìƒì„± ë° ì‹¤í–‰
    pipeline = create_pipeline(use_spark=use_spark, config=DEFAULT_ETL_CONFIG)
    
    # ë¹„ë™ê¸° ì‹¤í–‰
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    
    try:
        results = loop.run_until_complete(pipeline.run(incremental=incremental))
        
        # ê²°ê³¼ ë¡œê¹…
        for step, result in results.items():
            if result.success:
                logger.info(f"{step}: {result.records_processed} processed, {result.processing_time_seconds:.2f}s")
            else:
                logger.error(f"{step} failed: {result.error_message}")
        
        # ì „ì²´ ì„±ê³µ ì—¬ë¶€ í™•ì¸
        overall_success = all(r.success for r in results.values())
        if not overall_success:
            raise Exception("One or more ETL steps failed")
        
        return {
            "success": True,
            "results": {k: v.__dict__ for k, v in results.items()},
            "execution_date": execution_date.isoformat()
        }
        
    finally:
        loop.close()


def check_data_quality(**context) -> Dict[str, Any]:
    """ë°ì´í„° í’ˆì§ˆ ê²€ì¦ íƒœìŠ¤í¬."""
    from analytics.core.database import get_analytics_db
    
    logger.info("Starting data quality checks")
    
    conn = get_analytics_db()
    
    # ê¸°ë³¸ ë°ì´í„° í’ˆì§ˆ ì²´í¬
    checks = {
        "customer_analytics_count": "SELECT COUNT(*) FROM customer_analytics",
        "visit_analytics_count": "SELECT COUNT(*) FROM visit_analytics", 
        "service_preferences_count": "SELECT COUNT(*) FROM customer_service_preferences",
        "service_tags_count": "SELECT COUNT(*) FROM customer_service_tags",
        "null_customer_names": "SELECT COUNT(*) FROM customer_analytics WHERE name IS NULL",
        "duplicate_customers": """
            SELECT COUNT(*) FROM (
                SELECT customer_id, COUNT(*) as cnt 
                FROM customer_analytics 
                GROUP BY customer_id 
                HAVING COUNT(*) > 1
            )
        """
    }
    
    results = {}
    
    for check_name, query in checks.items():
        try:
            result = conn.execute(query).fetchone()
            count = result[0] if result else 0
            results[check_name] = count
            logger.info(f"{check_name}: {count}")
        except Exception as e:
            logger.error(f"Quality check {check_name} failed: {e}")
            results[check_name] = -1
    
    # í’ˆì§ˆ ê¸°ì¤€ ê²€ì¦
    quality_issues = []
    
    if results.get("null_customer_names", 0) > 0:
        quality_issues.append(f"Found {results['null_customer_names']} customers with null names")
    
    if results.get("duplicate_customers", 0) > 0:
        quality_issues.append(f"Found {results['duplicate_customers']} duplicate customers")
    
    if quality_issues:
        logger.warning(f"Data quality issues found: {quality_issues}")
        # í’ˆì§ˆ ì´ìŠˆê°€ ìˆì–´ë„ ì‹¤íŒ¨ì‹œí‚¤ì§€ ì•ŠìŒ (ê²½ê³ ë§Œ)
    
    return {
        "success": True,
        "quality_checks": results,
        "quality_issues": quality_issues
    }


def send_notification(**context) -> Dict[str, Any]:
    """ì•Œë¦¼ ì „ì†¡ íƒœìŠ¤í¬."""
    task_instance = context['task_instance']
    dag_run = context['dag_run']
    
    # ì´ì „ íƒœìŠ¤í¬ ê²°ê³¼ í™•ì¸
    etl_result = task_instance.xcom_pull(task_ids='run_etl_pipeline')
    quality_result = task_instance.xcom_pull(task_ids='check_data_quality')
    
    # ë””ìŠ¤ì½”ë“œ ë©”ì‹œì§€ í¬ë§·íŒ…
    message = f"""
    **ETL Pipeline ì‹¤í–‰ ê²°ê³¼**
    
    ğŸ“… ì‹¤í–‰ ì¼ì‹œ: {dag_run.execution_date}
    âœ… ETL ì„±ê³µ ì—¬ë¶€: {etl_result.get('success', False) if etl_result else False}
    ğŸ” í’ˆì§ˆ ê²€ì‚¬: {len(quality_result.get('quality_issues', [])) if quality_result else 0} ì´ìŠˆ ë°œê²¬
    
    **ì²˜ë¦¬ ê²°ê³¼**
    """
    
    # ETL ê²°ê³¼ ìƒì„¸ ì •ë³´ ì¶”ê°€
    if etl_result and 'results' in etl_result:
        for step, result in etl_result['results'].items():
            message += f"\nâ€¢ {step}: {result.get('records_processed', 0)} ê±´ ì²˜ë¦¬ ({result.get('processing_time_seconds', 0):.2f}ì´ˆ)"
    
    # í’ˆì§ˆ ì´ìŠˆê°€ ìˆë‹¤ë©´ ì¶”ê°€
    if quality_result and quality_result.get('quality_issues'):
        message += "\n\n**ë°œê²¬ëœ í’ˆì§ˆ ì´ìŠˆ**"
        for issue in quality_result['quality_issues']:
            message += f"\nâ— {issue}"
    
    logger.info(f"Sending Discord notification")
    
    # ë””ìŠ¤ì½”ë“œ ì›¹í›… URL ê°€ì ¸ì˜¤ê¸°
    webhook_url = os.getenv('DISCORD_WEBHOOK_URL')
    if not webhook_url:
        logger.warning("Discord webhook URL not found in environment variables")
        return {"success": False, "error": "Discord webhook URL not configured"}
    
    try:
        # ë””ìŠ¤ì½”ë“œë¡œ ë©”ì‹œì§€ ì „ì†¡
        response = requests.post(
            webhook_url,
            json={
                "content": message,
                "username": "ETL Pipeline Bot",
                "avatar_url": "https://i.imgur.com/4M34hi2.png"
            }
        )
        response.raise_for_status()
        logger.info("Discord notification sent successfully")
        return {"success": True, "message": message}
        
    except Exception as e:
        logger.error(f"Failed to send Discord notification: {e}")
        return {"success": False, "error": str(e)}


def create_customer_analytics_dag():
    """ê³ ê° ë¶„ì„ ETL DAG ìƒì„±."""
    if not AIRFLOW_AVAILABLE:
        logger.warning("Airflow not available. DAG creation skipped.")
        return None
    
    # DAG ê¸°ë³¸ ì„¤ì •
    default_args = DEFAULT_AIRFLOW_CONFIG.to_airflow_args()
    default_args.update({
        'owner': 'deveagles-analytics',
        'email_on_failure': True,
        'email_on_retry': False,
        'email': ['64etuor@gmail.com'],
    })
    
    dag = DAG(
        'customer_analytics_etl',
        default_args=default_args,
        description='Customer Analytics ETL Pipeline',
        schedule_interval='0 2 * * *',  # ë§¤ì¼ ìƒˆë²½ 2ì‹œ
        start_date=days_ago(1),
        catchup=False,
        max_active_runs=1,
        tags=['analytics', 'etl', 'customer'],
    )
    
    # 1. ë°ì´í„° ê°€ìš©ì„± ì²´í¬ (ì„ íƒì‚¬í•­)
    check_crm_data = BashOperator(
        task_id='check_crm_data_availability',
        bash_command='echo "Checking CRM data availability..."',
        dag=dag,
    )
    
    # 2. ETL íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    run_etl = PythonOperator(
        task_id='run_etl_pipeline',
        python_callable=run_etl_pipeline,
        dag=dag,
    )
    
    # 3. ë°ì´í„° í’ˆì§ˆ ê²€ì¦
    quality_check = PythonOperator(
        task_id='check_data_quality',
        python_callable=check_data_quality,
        dag=dag,
    )
    
    # 4. ì•Œë¦¼ ì „ì†¡
    notify = PythonOperator(
        task_id='send_notification',
        python_callable=send_notification,
        dag=dag,
    )
    
    # íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì •
    check_crm_data >> run_etl >> quality_check >> notify
    
    return dag


def create_spark_etl_dag():
    """Spark ê¸°ë°˜ ëŒ€ìš©ëŸ‰ ETL DAG ìƒì„±."""
    if not AIRFLOW_AVAILABLE:
        logger.warning("Airflow not available. DAG creation skipped.")
        return None
    
    # Spark ì „ìš© ì„¤ì •
    spark_args = DEFAULT_AIRFLOW_CONFIG.to_airflow_args()
    spark_args.update({
        'owner': 'deveagles-analytics',
        'execution_timeout': timedelta(hours=2),  # Spark ì‘ì—…ì€ ë” ê¸´ íƒ€ì„ì•„ì›ƒ
        'email_on_failure': True,
        'email': ['analytics@deveagles.com'],
    })
    
    dag = DAG(
        'customer_analytics_spark_etl',
        default_args=spark_args,
        description='Customer Analytics Spark ETL Pipeline (Large Data)',
        schedule_interval='0 1 * * 0',  # ë§¤ì£¼ ì¼ìš”ì¼ ìƒˆë²½ 1ì‹œ (ì£¼ê°„ ì „ì²´ ì²˜ë¦¬)
        start_date=days_ago(1),
        catchup=False,
        max_active_runs=1,
        tags=['analytics', 'etl', 'spark', 'weekly'],
    )
    
    # Spark ETL ì‹¤í–‰ (ì „ì²´ ë°ì´í„° ì²˜ë¦¬)
    run_spark_etl = PythonOperator(
        task_id='run_spark_etl_pipeline',
        python_callable=run_etl_pipeline,
        op_kwargs={'use_spark': True, 'incremental': False},
        dag=dag,
    )
    
    # í’ˆì§ˆ ê²€ì¦
    quality_check = PythonOperator(
        task_id='check_data_quality',
        python_callable=check_data_quality,
        dag=dag,
    )
    
    # ì•Œë¦¼
    notify = PythonOperator(
        task_id='send_notification',
        python_callable=send_notification,
        dag=dag,
    )
    
    # íƒœìŠ¤í¬ ì˜ì¡´ì„±
    run_spark_etl >> quality_check >> notify
    
    return dag


# DAG ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (Airflowê°€ ìë™ìœ¼ë¡œ ê°ì§€)
if AIRFLOW_AVAILABLE:
    # ì¼ì¼ ì¦ë¶„ ETL
    daily_etl_dag = create_customer_analytics_dag()
    
    # ì£¼ê°„ ì „ì²´ ETL (Spark)
    weekly_spark_dag = create_spark_etl_dag()
    
    # ê¸€ë¡œë²Œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— DAG ë“±ë¡
    globals()['customer_analytics_etl'] = daily_etl_dag
    globals()['customer_analytics_spark_etl'] = weekly_spark_dag
else:
    logger.warning("Airflow not available. DAGs not created.")


# ìˆ˜ë™ ì‹¤í–‰ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜ë“¤
def run_etl_manually(use_spark: bool = False, incremental: bool = True):
    """ETL íŒŒì´í”„ë¼ì¸ ìˆ˜ë™ ì‹¤í–‰."""
    import asyncio
    
    logger.info(f"Running ETL manually - spark: {use_spark}, incremental: {incremental}")
    
    pipeline = create_pipeline(use_spark=use_spark, config=DEFAULT_ETL_CONFIG)
    
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    
    try:
        results = loop.run_until_complete(pipeline.run(incremental=incremental))
        
        for step, result in results.items():
            if result.success:
                print(f"âœ“ {step}: {result.records_processed} processed ({result.processing_time_seconds:.2f}s)")
            else:
                print(f"âœ— {step}: {result.error_message}")
        
        return results
        
    finally:
        loop.close()


if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œ ìˆ˜ë™ í…ŒìŠ¤íŠ¸
    print("Running ETL pipeline manually...")
    results = run_etl_manually(use_spark=False, incremental=True)
    print(f"Completed with {len([r for r in results.values() if r.success])} successful steps") 